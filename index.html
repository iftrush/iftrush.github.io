<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kan Jen Cheng</title>

    <meta name="author" content="Kan Jen Cheng">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="images/eecs_logo.jpg">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Kan Jen Cheng
                </p>
                <p>I'm a student at <a href="https://www.berkeley.edu/">UC Berkeley</a>. Currently, I am doing audio research in <a href="https://people.eecs.berkeley.edu/~gopala/">Berkeley Speech Group</a>.
                </p>
                <p>
                  My research interests center on auditory perception, sound synthesis, texture editing, and computer vision. Recognizing that human perception of the environment relies heavily on the interplay between auditory and visual cues, I aim to develop sophisticated multi-modal systems capable of integrating audio-visual information to enhance human understanding, interpretation, and interaction with the world.
                </p>
                <p style="text-align:center">
                  <a href="mailto:kanjencheng@berkeley.edu">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Scholar</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/iftrush/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href=""><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="./images/profilepic.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in deep learning, generative AI, and audio processing. Most of my research is about inferring the physical world (speech, sound etc) from audio. Some papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



    <tr>
      <td style="padding:20px;width:45%;margin: auto;text-align: center;">
        <div class="one" style="margin-left: 0; position: relative">
          <img src="images/emo_score.png" id="emo_score_img" style="opacity:0; position:absolute; top:-10%; left:50%; width:130%; height:130%; transition: opacity 0.2s;">
          <img src="images/emo_reason_teaser.png" class="responsive-image-big" id="emo_img" style="margin-top: 0px; cursor:default; transition: opacity 0.2s;">
        </div>
        <script type="text/javascript">
          const emoImg = document.getElementById('emo_img');
          const emoScoreImg = document.getElementById('emo_score_img');
          emoImg.addEventListener('mouseover', () => {
            emoScoreImg.style.opacity = "1";
            emoImg.style.opacity = "0";
          });
          emoImg.addEventListener('mouseout', () => {
            emoScoreImg.style.opacity = "0";
            emoImg.style.opacity = "1";
          });
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://berkeley-speech-group.github.io/audio-texture-analogy/">
          <span class="papertitle">EMO-Reasoning: Benchmarking Emotional Reasoning Capabilities in Spoken Dialogue Systems</span>
        </a>
        <br>
        Jingwen Liu*,
				<a href="https://iftrush.github.io/">Kan Jen Cheng*</a>,
        <a href="https://jlian2.github.io/">Jiachen Lian</a>,
        Akshay Anand,
        Rishi Jain,
        Faith Qiao,
        <a href="https://www.stat.berkeley.edu/~yugroup/people/Robbie.html">Robin Netzorg</a>,
        <a href="https://hcchou.wixsite.com/huangchengchou">Huang-Cheng Chou</a>,
        <a href="https://tinglok.netlify.app/">Tingle Li</a>,
        <a href="https://daniellin94144.github.io/">Guan-Ting Lin</a>,
        <a href="https://people.eecs.berkeley.edu/~gopala/">Gopala Anumanchipalli</a>
        <br>
          <em>ASRU</em>, 2025 &nbsp <font color="red"></font>
        <br>
        <a href="">project page</a>
        /
        <a href="">arXiv</a>
        <p></p>
        <p>
          A holistic benchmark for assessing emotional coherence in spoken dialogue systems through continuous, categorical, and perceptual metrics.</p>
      </td>
    </tr>


    <tr onmouseout="flash_cache_stop()" onmouseover="flash_cache_start()">
      <td style="padding:20px;width:45%;margin: auto;text-align: center;">
        <div class="one" style="margin-left: 0;">
          <div class="two" id='flash_cache_image'><video  width=150% muted autoplay loop class="responsive-image">
          <source src="images/audio_texture_editing.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/audio_texture_editing_fix.png' class="responsive-image-big" id="ate" style="margin-top: 40px">
        </div>
        <script type="text/javascript">
          function flash_cache_start() {
            document.getElementById('flash_cache_image').style.opacity = "1";
            document.getElementById('ate').style.opacity = "0";
          }

          function flash_cache_stop() {
            document.getElementById('flash_cache_image').style.opacity = "0";
            document.getElementById('ate').style.opacity = "1";
          }
          flash_cache_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://berkeley-speech-group.github.io/audio-texture-analogy/">
          <span class="papertitle">Audio Texture Manipulation by Exemplar-Based Analogy</span>
        </a>
        <br>
				<a href="https://iftrush.github.io/">Kan Jen Cheng*</a>,
        <a href="https://tinglok.netlify.app/">Tingle Li*</a>,
        <a href="https://people.eecs.berkeley.edu/~gopala/">Gopala Anumanchipalli</a>
        <br>
          <em>ICASSP</em>, 2025 &nbsp <font color="red"></font>
        <br>
        <a href="https://berkeley-speech-group.github.io/audio-texture-analogy/">project page</a>
        /
        <a href="https://arxiv.org/abs/2501.12385">arXiv</a>
        <p></p>
        <p>
          An exemplar-based analogy model for audio texture manipulation that uses paired speech examples to learn transformations.</p>
      </td>
    </tr>



            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Last updated: <span id="last-updated"></span>
                </p>

                <p style="text-align:right;font-size:small;">
                  Template from <a href="https://jonbarron.info/">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>

    <script>
      document.addEventListener("DOMContentLoaded", () => {
          const lastUpdated = new Date(document.lastModified);
          const options = { year: 'numeric', month: 'short' };
          document.getElementById("last-updated").textContent = 
              `${lastUpdated.toLocaleDateString('en-US', options)}`;
      });
    </script>
  

  </body>
</html>
